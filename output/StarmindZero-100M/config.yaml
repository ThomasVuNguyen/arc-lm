pretrain:
  dataset:
    - name: ThomasTheMaker/arc-fineweb
      rows: 1000
  training:
    tokenizer: "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
    batch_size: 1
    grad_accum_steps: 2
    num_epochs: 1
    learning_rate: 3.0e-4
    weight_decay: 0.1
    warmup_ratio: 0.01
    grad_clip: 1.0
    log_every: 50
    save_every: 5000
    random_seed: 42
  model:
    name: 'StarmindZero-100M'
    architecture: 'LlamaForCausalLM'
    config:
      vocab_size: 32000
      hidden_size: 768
      num_hidden_layers: 12
      num_attention_heads: 12
      intermediate_size: 3072
      max_position_embeddings: 2048
      rms_norm_eps: 1e-5
      initializer_range: 0.02
      use_cache: false
